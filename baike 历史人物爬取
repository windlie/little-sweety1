from bs4 import BeautifulSoup
import re
import urlparse
import urllib2
import os
import sys   
reload(sys)   
sys.setdefaultencoding('utf8')     
import json

class UrlManager(object):
    def __init__(self):
        self.new_urls = set()
        self.old_urls = set()
        
    def add_new_url(self,url):
        if url is None:
            raise Exception 
        if url not in self.new_urls and url not in self.old_urls:
            self.new_urls.add(url)
 
    def add_new_urls(self,urls):
        if urls is None or len(urls) == 0:
            raise Exception 
        for url in urls:
            self.add_new_url(url)

    def has_new_url(self):
        return len(self.new_urls) != 0

    def get_new_url(self):
        new_url = self.new_urls.pop()
        self.old_urls.add(new_url)
        return new_url

class HtmlDownloader(object):

    def download(self, url):
        if url is None:
            return None
        response = urllib2.urlopen(url)

        if response.getcode() != 200:
            return None
        return response.read()



class HtmlParser(object):
    def _get_new_data(self,page_url,soup):
        res_data = {}
        res_data["url"] = page_url 
        data = soup.find_all("div",class_ = "list")                 #find标签大类
        
        data1 = []
        for info in data:
            res_data1 = {}
        
            title = info.find('a')                                      #找到title对应的标签
            res_data1['title'] = title.get_text()
            
            summary = info.find('p')                                         #找到summary对应标签
            res_data1['summary'] = summary.get_text()
            data1.append(res_data1)
   
        res_data['name'] = data1
        return res_data

    def parse(self,page_url,html_cont):
        if page_url is None or html_cont is None:
            return

        soup = BeautifulSoup(html_cont,"html.parser", from_encoding="utf-8")
        new_data = self._get_new_data(page_url,soup)
        return new_data


class HtmlOutputer():
    def __init__(self):
        self.data = []

    def collect_data(self, data):
        if data is None:
            return
        self.data.append(data)

    def output_html(self):
        jsonStr = json.dumps(self.data,ensure_ascii = False, indent = 4)         #json.dumps  将 Python 对象编码成 JSON 
        print jsonStr
        with open("output.json", "w") as f:                                     #以JSON形式输出
            f.write(jsonStr)
            f.close() 

class SpiderMain():
    def craw(self, root_url):
        count = 1   
        UrlManager.add_new_url(root_url)
        while UrlManager.has_new_url():   

            try:
                new_url =UrlManager.get_new_url()
                print "\n crawed %d :%s" % (count, new_url)
                html_cont =HtmlDownloader.download(new_url)
                new_data =HtmlParser.parse(new_url, html_cont)
                #UrlManager.add_new_urls(new_urls)
                HtmlOutputer.collect_data(new_data)
                count = count + 1
            except Exception as e:
                print('craw failed',e)
        
        HtmlOutputer.output_html()

 

if __name__=="__main__":
    print "\n Welcome to use spider:)"

    UrlManager = UrlManager()
    HtmlDownloader = HtmlDownloader()
    HtmlParser = HtmlParser()
    HtmlOutputer = HtmlOutputer()
    SpiderMain = SpiderMain()
   
    for a in range(1,10):                       #设置循环页数和URL

        
        root_url = 'http://baike.baidu.com/fenlei/%E8%AF%97%E4%BA%BA?limit=30&index={}&offset=30#gotoList'.format(a)
        print "root_url = ", root_url
        SpiderMain.craw(root_url)

    
